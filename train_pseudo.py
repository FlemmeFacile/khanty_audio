# train_pseudo.py
import os
import json
import torch
from torch.utils.data import DataLoader
from models import SynthesizerTrn
from data_utils import get_dataset
from torch.nn.utils import clip_grad_norm_

# ------------------------------
# 1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
# ------------------------------
with open("configs/fi_pseudo_pretrain.json", "r", encoding="utf-8") as f:
    config = json.load(f)

train_config = config["train"]
data_config = config["data"]
model_config = config["model"]

# ------------------------------
# 2Ô∏è‚É£ –î–∞—Ç–∞—Å–µ—Ç
# ------------------------------
class HParams:
    def __init__(self):
        self.text_cleaners = data_config.get("text_cleaners", [])
        self.max_wav_value = data_config.get("max_wav_value", 32768.0)
        self.sampling_rate = data_config.get("sampling_rate", 22050)
        self.filter_length = data_config.get("filter_length", 1024)
        self.hop_length = data_config.get("hop_length", 256)
        self.win_length = data_config.get("win_length", 1024)
        self.n_mel_channels = data_config.get("n_mel_channels", 80)
        self.mel_fmin = data_config.get("mel_fmin", 0.0)
        self.mel_fmax = data_config.get("mel_fmax", None)
        self.add_blank = data_config.get("add_blank", False)           
        self.min_text_len = data_config.get("min_text_len", 1)       
        self.max_text_len = data_config.get("max_text_len", 1000)    
        self.segment_size = train_config["segment_size"]
        self.n_speakers = 0 
        self.distributed_run = False 
        self.use_pseudo_text_encoder = True 
        self.n_speakers = 0
        self.distributed_run = False
        self.world_size = 1
        self.rank = 0 


hparams = HParams()
train_dataset, train_loader = get_dataset(
    data_config["training_files"], 
    hparams,           
    num_workers=0, 
    batch_size=train_config["batch_size"]
)
val_dataset, val_loader = get_dataset(
    data_config["validation_files"], 
    hparams, 
    num_workers=0, 
    batch_size=train_config["batch_size"]
)

print(f"‚úÖ Train batches: {len(train_loader)}")  # üî• len(LOADER)!
print(f"‚úÖ Val batches: {len(val_loader)}")

# ------------------------------
# 3Ô∏è‚É£ –ú–æ–¥–µ–ª—å
# ------------------------------
synth = SynthesizerTrn(
    n_vocab=model_config['n_vocab'],
    spec_channels=model_config['spec_channels'],
    segment_size=train_config['segment_size'],
    inter_channels=model_config['inter_channels'],
    hidden_channels=model_config['hidden_channels'],
    filter_channels=model_config['filter_channels'],
    n_heads=model_config['n_heads'],
    n_layers=model_config['n_layers'],
    kernel_size=model_config['kernel_size'],
    p_dropout=model_config['p_dropout'],
    resblock=model_config['resblock'],
    resblock_kernel_sizes=model_config['resblock_kernel_sizes'],
    resblock_dilation_sizes=model_config['resblock_dilation_sizes'],
    upsample_rates=model_config['upsample_rates'],
    upsample_initial_channel=model_config['upsample_initial_channel'],
    upsample_kernel_sizes=model_config['upsample_kernel_sizes'],
    n_speakers=model_config.get('n_speakers', 0),
    gin_channels=model_config.get('gin_channels', 0),
    use_sdp=True,
    use_pseudo_text_encoder=model_config.get('use_pseudo_text_encoder', False),
    pseudo_text_encoder_params=model_config.get('pseudo_text_encoder', {})
)

device = "cuda" if torch.cuda.is_available() else "cpu"
synth.to(device)

# ------------------------------
# 4Ô∏è‚É£ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
# ------------------------------
optimizer = torch.optim.Adam(
    synth.parameters(),
    lr=train_config["learning_rate"],
    betas=train_config["betas"],
    eps=train_config["eps"]
)

# ------------------------------
# 5Ô∏è‚É£ –ß–µ–∫–ø–æ–∏–Ω—Ç
# ------------------------------
checkpoint_dir = "checkpoints"
os.makedirs(checkpoint_dir, exist_ok=True)

def save_checkpoint(model, optimizer, epoch, path):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()
    }, path)
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {path}")

def load_checkpoint(model, optimizer, path):
    checkpoint = torch.load(path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    if optimizer:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {path}")
    return checkpoint['epoch']

# ------------------------------
# 6Ô∏è‚É£ –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏
# ------------------------------
def train_one_epoch(model, dataloader, optimizer, device, train=True):
    model.train() if train else model.eval()
    total_loss = 0.0
    
    for batch in dataloader:
        # ‚úÖ –ü–æ–ª–Ω—ã–π VITS batch
        x, x_lengths, spec, spec_lengths, y, y_lengths, spk_ids = [b.to(device) for b in batch]
        
        if train:
            optimizer.zero_grad()
        
        # ‚úÖ –ü–æ–ª–Ω—ã–π VITS forward
        o, l_length, attn, ids_slice, x_mask, y_mask, *_ = model(
            x, x_lengths, spec, spec_lengths, y, y_lengths, spk_ids
        )
        
        # ‚úÖ VITS duration loss (–Ω–∞ —Å—Ç–∞—Ä—Ç–µ)
        loss = l_length.mean()
        
        if train:
            loss.backward()
            clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
        
        total_loss += loss.item()
    
    return total_loss / len(dataloader)


# ------------------------------
# 7Ô∏è‚É£ –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
# ------------------------------
start_epoch = 1
latest_checkpoint = os.path.join(checkpoint_dir, "latest.pt")
if os.path.exists(latest_checkpoint):
    start_epoch = load_checkpoint(synth, optimizer, latest_checkpoint) + 1

num_epochs = train_config["epochs"]
log_interval = train_config.get("log_interval", 1)
eval_interval = train_config.get("eval_interval", 5)

for epoch in range(start_epoch, num_epochs + 1):
    train_loss = train_one_epoch(synth, train_loader, optimizer, device, train=True)
    
    if epoch % log_interval == 0:
        print(f"[Epoch {epoch}] train_loss: {train_loss:.6f}")
    
    if epoch % eval_interval == 0:
        val_loss = train_one_epoch(synth, val_loader, optimizer=None, device=device, train=False)
        print(f"[Epoch {epoch}] val_loss: {val_loss:.6f}")
    
    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è
    save_checkpoint(synth, optimizer, epoch, latest_checkpoint)

    # —Å–æ—Ö—Ä–∞–Ω—è–µ–º "–≤–µ—á–Ω—ã–π" —á–µ–∫–ø–æ–∏–Ω—Ç –∫–∞–∂–¥—ã–µ 1000 —ç–ø–æ—Ö
    if epoch % 1000 == 0:
        checkpoint_path = os.path.join(checkpoint_dir, f"synth_epoch_{epoch}.pt")
        save_checkpoint(synth, optimizer, epoch, checkpoint_path)