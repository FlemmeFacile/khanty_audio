# train_pseudo.py
import os
import json
import torch
from torch.utils.data import DataLoader
from models import SynthesizerTrn
from data_utils import get_dataset
from torch.nn.utils import clip_grad_norm_

import random
import shutil

# ------------------------------
# 1Ô∏è‚É£ –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
# ------------------------------
with open("configs/fi_pseudo_pretrain.json", "r", encoding="utf-8") as f:
    config = json.load(f)

train_config = config["train"]
data_config = config["data"]
model_config = config["model"]

# === –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–Ω–∞–±–æ—Ä–∞ ===
subset_dir = r"H:\tts\fin\subset_wav_22050"
source_dir = data_config["training_files_dir"]  # –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–æ–±–∞–≤—å —ç—Ç–æ—Ç –∫–ª—é—á –≤ json
num_files = 1000  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –ø–æ–¥–Ω–∞–±–æ—Ä

os.makedirs(subset_dir, exist_ok=True)
all_files = [f for f in os.listdir(source_dir) if f.endswith(".wav")]
subset_files = random.sample(all_files, num_files)

for f in subset_files:
    shutil.copy(os.path.join(source_dir, f), os.path.join(subset_dir, f))

print(f"‚úÖ –ü–æ–¥–Ω–∞–±–æ—Ä –¥–ª—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏—è: {len(subset_files)} —Ñ–∞–π–ª–æ–≤")
# –º–µ–Ω—è–µ–º –ø—É—Ç—å –≤ –∫–æ–Ω—Ñ–∏–≥–µ –Ω–∞ –ø–æ–¥–Ω–∞–±–æ—Ä
data_config["training_files"] = subset_dir

# ------------------------------
# 2Ô∏è‚É£ –î–∞—Ç–∞—Å–µ—Ç
# ------------------------------
class HParams:
    def __init__(self):
        self.text_cleaners = data_config.get("text_cleaners", [])
        self.max_wav_value = data_config.get("max_wav_value", 32768.0)
        self.sampling_rate = data_config.get("sampling_rate", 22050)
        self.filter_length = data_config.get("filter_length", 1024)
        self.hop_length = data_config.get("hop_length", 256)
        self.win_length = data_config.get("win_length", 1024)
        self.n_mel_channels = data_config.get("n_mel_channels", 80)
        self.mel_fmin = data_config.get("mel_fmin", 0.0)
        self.mel_fmax = data_config.get("mel_fmax", None)
        self.add_blank = data_config.get("add_blank", False)           
        self.min_text_len = data_config.get("min_text_len", 1)       
        self.max_text_len = data_config.get("max_text_len", 1000)    
        self.segment_size = train_config["segment_size"]
        self.n_speakers = 0
        self.distributed_run = False
        self.use_pseudo_text_encoder = True
        self.world_size = 1
        self.rank = 0 

hparams = HParams()
train_dataset, train_loader = get_dataset(
    data_config["training_files"], 
    hparams,           
    num_workers=0, 
    batch_size=train_config["batch_size"]
)
val_dataset, val_loader = get_dataset(
    data_config["validation_files"], 
    hparams, 
    num_workers=0, 
    batch_size=train_config["batch_size"]
)

print(f"‚úÖ Train batches: {len(train_loader)}")
print(f"‚úÖ Val batches: {len(val_loader)}")

# ------------------------------
# 3Ô∏è‚É£ –ú–æ–¥–µ–ª—å
# ------------------------------
synth = SynthesizerTrn(
    n_vocab=model_config['n_vocab'],
    spec_channels=model_config['spec_channels'],
    segment_size=train_config['segment_size'],
    inter_channels=model_config['inter_channels'],
    hidden_channels=model_config['hidden_channels'],
    filter_channels=model_config['filter_channels'],
    n_heads=model_config['n_heads'],
    n_layers=model_config['n_layers'],
    kernel_size=model_config['kernel_size'],
    p_dropout=model_config['p_dropout'],
    resblock=model_config['resblock'],
    resblock_kernel_sizes=model_config['resblock_kernel_sizes'],
    resblock_dilation_sizes=model_config['resblock_dilation_sizes'],
    upsample_rates=model_config['upsample_rates'],
    upsample_initial_channel=model_config['upsample_initial_channel'],
    upsample_kernel_sizes=model_config['upsample_kernel_sizes'],
    n_speakers=model_config.get('n_speakers', 0),
    gin_channels=model_config.get('gin_channels', 0),
    use_sdp=True,
    use_pseudo_text_encoder=model_config.get('use_pseudo_text_encoder', False),
    pseudo_text_encoder_params=model_config.get('pseudo_text_encoder', {})
)

device = "cuda" if torch.cuda.is_available() else "cpu"
synth.to(device)

# ------------------------------
# 4Ô∏è‚É£ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
# ------------------------------
optimizer = torch.optim.Adam(
    synth.parameters(),
    lr=train_config["learning_rate"],
    betas=train_config["betas"],
    eps=train_config["eps"]
)

# ------------------------------
# 5Ô∏è‚É£ –ß–µ–∫–ø–æ–∏–Ω—Ç
# ------------------------------
checkpoint_dir = "checkpoints"
os.makedirs(checkpoint_dir, exist_ok=True)

def save_checkpoint(model, optimizer, epoch, path):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict()
    }, path)
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω —á–µ–∫–ø–æ–∏–Ω—Ç: {path}")

def load_checkpoint(model, optimizer, path):
    checkpoint = torch.load(path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    if optimizer:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {path}")
    return checkpoint['epoch']

# ------------------------------
# 6Ô∏è‚É£ –§—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏
# ------------------------------
def train_one_epoch(model, dataloader, optimizer, device, epoch, train=True, checkpoint_dir="checkpoints", checkpoint_interval=1000):
    model.train() if train else model.eval()
    total_loss = 0.0

    for step, batch in enumerate(dataloader, start=1):
        x, x_lengths, spec, spec_lengths, y, y_lengths, spk_ids = [b.to(device) for b in batch]

        if train:
            optimizer.zero_grad()

        o, l_length, attn, ids_slice, x_mask, y_mask, *_ = model(
            x, x_lengths, spec, spec_lengths, y, y_lengths, spk_ids
        )

        loss = l_length.mean()

        if train:
            loss.backward()
            clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

        total_loss += loss.item()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç –∫–∞–∂–¥—ã–µ N —à–∞–≥–æ–≤
        if train and step % checkpoint_interval == 0:
            step_checkpoint = os.path.join(checkpoint_dir, f"checkpoint_epoch{epoch}_step{step}.pt")
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict()
            }, step_checkpoint)
            print(f"üíæ –°–æ—Ö—Ä–∞–Ω–∏–ª–∏ –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç: {step_checkpoint}")

    avg_loss = total_loss / len(dataloader)
    return avg_loss

# ------------------------------
# 7Ô∏è‚É£ –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
# ------------------------------
start_epoch = 1
latest_checkpoint = os.path.join(checkpoint_dir, "latest.pt")
if os.path.exists(latest_checkpoint):
    start_epoch = load_checkpoint(synth, optimizer, latest_checkpoint) + 1

num_epochs = train_config["epochs"]
log_interval = train_config.get("log_interval", 1)
eval_interval = train_config.get("eval_interval", 5)

for epoch in range(start_epoch, num_epochs + 1):
    train_loss = train_one_epoch(synth, train_loader, optimizer, device, epoch, train=True)
    
    if epoch % log_interval == 0:
        print(f"[Epoch {epoch}] train_loss: {train_loss:.6f}")
    
    if epoch % eval_interval == 0:
        val_loss = train_one_epoch(synth, val_loader, optimizer=None, device=device, epoch=epoch, train=False)
        print(f"[Epoch {epoch}] val_loss: {val_loss:.6f}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–∏–Ω—Ç
    save_checkpoint(synth, optimizer, epoch, latest_checkpoint)

    # "–í–µ—á–Ω—ã–π" —á–µ–∫–ø–æ–∏–Ω—Ç –∫–∞–∂–¥—ã–µ 1000 —ç–ø–æ—Ö
    if epoch % 1000 == 0:
        checkpoint_path = os.path.join(checkpoint_dir, f"synth_epoch_{epoch}.pt")
        save_checkpoint(synth, optimizer, epoch, checkpoint_path)
